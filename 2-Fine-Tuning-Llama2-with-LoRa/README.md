# Fine-Tuning Llama-2 LLM with LoRa

## Goal
The goal of this project was to understand the fine-tuning process of a Large Language Model, specifically Llama 2.

## Base Model
The base model used here is from HuggingFace - [NousResearch/Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf).

## Dataset
The dataset used here is also from HuggingFace - [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)

## Conclusion
This project achieved its goal of helping me understand the nuances in LLM fine-tuning process.  
This will serve as a base work for other LLM fine-tuning experiments.  

## Reference
[Hugging Face documentation](https://huggingface.co/docs)
[Article-KDNuggets](https://www.kdnuggets.com/fine-tuning-llamav2-with-qlora-on-google-colab-for-free)
